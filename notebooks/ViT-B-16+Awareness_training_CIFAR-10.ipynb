{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efb9327",
   "metadata": {},
   "source": [
    "## Train Awareness with ViT-B-16 encoder on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480bbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import yaml\n",
    "import torch\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from awareness import awareness\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5b8301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d721bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_PATH = './checkpoints/vit-b-16_cifar-10/experiments/exp1_tl0.0082_ta0.9526/weights/best.pt'\n",
    "WINDOW_SIZE = 1\n",
    "#BATCH_SIZE = 1024\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "IMG_SIZE = 224\n",
    "DYNAMIC_RAY = True\n",
    "MODEL_NAME = 'Awareness+ViT-B-16'\n",
    "DATASET_NAME = 'CIFAR-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284570ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder model parameters: 86,567,656\n",
      "Awareness model parameters: 0\n"
     ]
    }
   ],
   "source": [
    "encoder_model = torch.load(ENCODER_PATH)\n",
    "encoder_model.eval().to(device)\n",
    "\n",
    "feature_extractor = create_feature_extractor(encoder_model, return_nodes=['encoder'])\n",
    "\n",
    "awareness_model = awareness.Awareness(learnable=True, dynamic_ray=True)\n",
    "awareness_model.to(device)\n",
    "\n",
    "print(\"Encoder model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in encoder_model.parameters()]):,}\")\n",
    "print(\"Awareness model parameters:\", f\"{int(np.sum([int(np.prod(p.shape)) for p in awareness_model.parameters()])):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf6d7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "cifar10_train = CIFAR10(os.path.expanduser(\"~/.cache\"), train=True, transform=preprocess, download=True)\n",
    "cifar10_test = CIFAR10(os.path.expanduser(\"~/.cache\"), train=False, transform=preprocess, download=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    cifar10_test,\n",
    "    batch_size=int(BATCH_SIZE),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79599bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (27 refs) --> 1.0\n",
      "Train (48 refs) --> 1.0\n",
      "Train (66 refs) --> 0.9375\n",
      "Train (79 refs) --> 0.8125\n",
      "Train (94 refs) --> 0.875\n",
      "Train (109 refs) --> 0.9688\n",
      "Train (124 refs) --> 0.9375\n",
      "Train (143 refs) --> 0.875\n",
      "Train (159 refs) --> 0.9375\n",
      "Train (176 refs) --> 0.9375\n",
      "Train (193 refs) --> 0.9375\n",
      "Train (204 refs) --> 0.9375\n",
      "Train (222 refs) --> 0.9688\n",
      "Train (236 refs) --> 0.9375\n",
      "Train (248 refs) --> 0.9062\n",
      "Train (260 refs) --> 0.9062\n",
      "Train (274 refs) --> 0.9375\n",
      "Train (290 refs) --> 0.9062\n",
      "Train (304 refs) --> 0.9062\n",
      "Train (316 refs) --> 0.8438\n",
      "Train (332 refs) --> 0.8125\n",
      "Train (349 refs) --> 0.9688\n",
      "Train (354 refs) --> 0.9062\n",
      "Train (369 refs) --> 0.9062\n",
      "Train (382 refs) --> 0.9688\n",
      "Train (393 refs) --> 0.9688\n",
      "Train (410 refs) --> 0.9375\n",
      "Train (419 refs) --> 0.9688\n",
      "Train (426 refs) --> 0.8438\n",
      "Train (438 refs) --> 0.8438\n",
      "Train (444 refs) --> 0.9062\n",
      "Train (455 refs) --> 0.9375\n",
      "Train (466 refs) --> 0.9062\n",
      "Train (473 refs) --> 0.9062\n",
      "Train (478 refs) --> 0.9062\n",
      "Train (489 refs) --> 0.9375\n",
      "Train (494 refs) --> 0.875\n",
      "Train (501 refs) --> 0.875\n",
      "Train (510 refs) --> 0.8438\n",
      "Train (516 refs) --> 0.875\n",
      "Train (527 refs) --> 0.8438\n",
      "Train (537 refs) --> 0.9688\n",
      "Train (546 refs) --> 0.9688\n",
      "Train (553 refs) --> 0.8438\n",
      "Train (567 refs) --> 0.9688\n",
      "Train (574 refs) --> 0.9688\n",
      "Train (583 refs) --> 0.9688\n",
      "Train (591 refs) --> 1.0\n",
      "Train (600 refs) --> 1.0\n",
      "Train (611 refs) --> 0.8125\n",
      "Train (620 refs) --> 0.875\n",
      "Train (637 refs) --> 0.9688\n",
      "Train (646 refs) --> 0.9062\n",
      "Train (655 refs) --> 0.8438\n",
      "Train (667 refs) --> 0.9688\n",
      "Train (682 refs) --> 0.9062\n",
      "Train (694 refs) --> 0.9688\n",
      "Train (713 refs) --> 0.9688\n",
      "Train (721 refs) --> 0.9688\n",
      "Train (732 refs) --> 0.8438\n",
      "Train (745 refs) --> 0.9688\n",
      "Train (757 refs) --> 0.9062\n",
      "Train (767 refs) --> 0.875\n",
      "Train (783 refs) --> 0.8438\n",
      "Train (795 refs) --> 0.9062\n",
      "Train (806 refs) --> 0.8125\n",
      "Train (820 refs) --> 0.9062\n",
      "Train (828 refs) --> 0.9375\n",
      "Train (842 refs) --> 0.9062\n",
      "Train (850 refs) --> 0.875\n",
      "Train (855 refs) --> 0.875\n",
      "Train (860 refs) --> 0.9375\n",
      "Train (872 refs) --> 0.9062\n",
      "Train (880 refs) --> 0.875\n",
      "Train (888 refs) --> 0.9062\n",
      "Train (903 refs) --> 0.9062\n",
      "Train (910 refs) --> 0.7812\n",
      "Train (923 refs) --> 0.9688\n",
      "Train (931 refs) --> 0.9062\n",
      "Train (939 refs) --> 0.9375\n",
      "Train (948 refs) --> 0.875\n",
      "Train (956 refs) --> 0.9375\n",
      "Train (964 refs) --> 0.9688\n",
      "Train (973 refs) --> 0.9375\n",
      "Train (982 refs) --> 1.0\n",
      "Train (989 refs) --> 1.0\n",
      "Train (1002 refs) --> 0.9688\n",
      "Train (1008 refs) --> 0.8438\n",
      "Train (1016 refs) --> 0.9688\n",
      "Train (1019 refs) --> 0.8438\n",
      "Train (1029 refs) --> 0.9375\n",
      "Train (1036 refs) --> 0.875\n",
      "Train (1046 refs) --> 0.8125\n",
      "Train (1054 refs) --> 0.9375\n",
      "Train (1066 refs) --> 0.875\n",
      "Train (1075 refs) --> 0.875\n",
      "Train (1084 refs) --> 0.9375\n",
      "Train (1089 refs) --> 0.875\n",
      "Train (1096 refs) --> 0.9062\n",
      "Train (1106 refs) --> 0.9062\n",
      "Train (1113 refs) --> 0.9062\n",
      "Train (1126 refs) --> 1.0\n",
      "Train (1129 refs) --> 0.8125\n",
      "Train (1142 refs) --> 0.9062\n",
      "Train (1152 refs) --> 0.9062\n",
      "Train (1163 refs) --> 0.9688\n",
      "Train (1175 refs) --> 0.9062\n",
      "Train (1184 refs) --> 0.8438\n",
      "Train (1193 refs) --> 0.9062\n",
      "Train (1201 refs) --> 0.9062\n",
      "Train (1212 refs) --> 0.9062\n",
      "Train (1221 refs) --> 0.875\n",
      "Train (1235 refs) --> 0.9688\n",
      "Train (1242 refs) --> 0.9062\n",
      "Train (1253 refs) --> 0.9062\n",
      "Train (1263 refs) --> 0.9375\n",
      "Train (1273 refs) --> 0.9375\n",
      "Train (1280 refs) --> 0.9688\n",
      "Train (1288 refs) --> 0.875\n",
      "Train (1298 refs) --> 0.9688\n",
      "Train (1308 refs) --> 0.9375\n",
      "Train (1319 refs) --> 0.9688\n",
      "Train (1328 refs) --> 0.9375\n",
      "Train (1339 refs) --> 0.875\n",
      "Train (1345 refs) --> 0.75\n",
      "Train (1356 refs) --> 0.875\n",
      "Train (1366 refs) --> 0.875\n",
      "Train (1374 refs) --> 0.9688\n",
      "Train (1392 refs) --> 0.9688\n",
      "Train (1403 refs) --> 0.875\n",
      "Train (1413 refs) --> 0.875\n",
      "Train (1424 refs) --> 0.9688\n",
      "Train (1438 refs) --> 1.0\n",
      "Train (1443 refs) --> 0.9062\n",
      "Train (1457 refs) --> 0.9688\n",
      "Train (1469 refs) --> 0.875\n",
      "Train (1475 refs) --> 0.9062\n",
      "Train (1486 refs) --> 0.9062\n",
      "Train (1496 refs) --> 0.9062\n",
      "Train (1506 refs) --> 0.9375\n",
      "Train (1518 refs) --> 0.9375\n",
      "Train (1530 refs) --> 0.9688\n",
      "Train (1540 refs) --> 0.9062\n",
      "Train (1546 refs) --> 0.8438\n",
      "Train (1556 refs) --> 0.7812\n",
      "Train (1563 refs) --> 0.9688\n",
      "Train (1569 refs) --> 0.875\n",
      "Train (1575 refs) --> 0.9062\n",
      "Train (1581 refs) --> 0.8125\n",
      "Train (1590 refs) --> 0.8125\n",
      "Train (1597 refs) --> 0.9062\n",
      "Train (1604 refs) --> 0.875\n",
      "Train (1615 refs) --> 0.9688\n",
      "Train (1625 refs) --> 0.9375\n",
      "Train (1632 refs) --> 0.9062\n",
      "Train (1641 refs) --> 0.875\n",
      "Train (1649 refs) --> 0.9375\n",
      "Train (1661 refs) --> 0.8438\n"
     ]
    }
   ],
   "source": [
    "save_path = f'./checkpoints/{MODEL_NAME}_{DATASET_NAME}'.lower()\n",
    "\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'encoder_path': ENCODER_PATH,\n",
    "    'dataset': DATASET_NAME,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'num_classes': NUM_CLASSES\n",
    "}\n",
    "\n",
    "results_data = {\n",
    "    'epoch': [], \n",
    "    'ref_instances': [],\n",
    "    'train_acc': [], \n",
    "    'test_acc': []\n",
    "}\n",
    "\n",
    "res_df = pd.DataFrame(results_data)\n",
    "\n",
    "if(not os.path.exists(f'{save_path}/')):\n",
    "    os.makedirs(f'{save_path}')\n",
    "\n",
    "with open(f'{save_path}/config.yaml', 'w') as yaml_file:\n",
    "    yaml.dump(config, yaml_file, default_flow_style=False)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_train_acc = 0.0\n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        cifar10_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    awareness_model.__init__(learnable=True, dynamic_ray=True)\n",
    "    \n",
    "    encoder_model.eval()\n",
    "    awareness_model.eval()\n",
    "    \n",
    "    train_loaders = [train_loader]\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "    \n",
    "        for train_loader in train_loaders:\n",
    "\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                \n",
    "                train_correct_preds_batches = []\n",
    "                test_correct_preds_batches = []\n",
    "\n",
    "                train_count = 0\n",
    "                test_count = 0\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    images = Variable(images.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "\n",
    "                #features = encoder_model(images).float()\n",
    "                features = torch.mean(feature_extractor(images)['encoder'].float(), 1)\n",
    "                \n",
    "                preds = awareness_model(torch.unsqueeze(features,1), set_labels=labels, update_ref_insts=True)\n",
    "\n",
    "                train_correct_preds_batch = np.sum(preds.cpu().numpy() == labels.cpu().numpy())\n",
    "                train_correct_preds_batches.append(train_correct_preds_batch)\n",
    "                train_count = train_count+len(images)\n",
    "\n",
    "                references = awareness_model.awareness.ref_insts\n",
    "                references_labels = awareness_model.awareness.ref_insts_labels\n",
    "\n",
    "                n_ref_insts = len(references)\n",
    "\n",
    "                train_acc = round(np.sum(train_correct_preds_batches)/train_count, 4)\n",
    "\n",
    "                print(f'Train ({n_ref_insts} refs) --> {train_acc}')\n",
    "        \n",
    "        print('####')\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                images = Variable(images.cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "\n",
    "            #features = encoder_model(images).float()\n",
    "            features = torch.mean(feature_extractor(images)['encoder'].float(), 1)\n",
    "            \n",
    "            preds = awareness_model(torch.unsqueeze(features,1))\n",
    "\n",
    "            test_correct_preds_batch = np.sum(preds.cpu().numpy() == labels.cpu().numpy())\n",
    "            test_correct_preds_batches.append(test_correct_preds_batch)\n",
    "            test_count = test_count+len(images)\n",
    "\n",
    "            test_acc = round(np.sum(test_correct_preds_batches)/test_count, 4)\n",
    "\n",
    "            print(f'Test ({n_ref_insts} refs) --> {test_acc}')\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Reference instances (N): {n_ref_insts}, Train accuracy: {train_acc}, Test accuracy: {test_acc}')\n",
    "\n",
    "        results_data = {\n",
    "            'epoch': epoch+1, \n",
    "            'ref_instances': n_ref_insts, \n",
    "            'train_acc': train_acc, \n",
    "            'test_acc': test_acc\n",
    "        }\n",
    "\n",
    "        if(not os.path.exists(f'{save_path}/weights')):\n",
    "            os.makedirs(f'{save_path}/weights')\n",
    "    \n",
    "        res_df.loc[len(res_df)] = results_data\n",
    "        res_df.to_csv(f'{save_path}/results.csv', index=False)\n",
    "    \n",
    "        torch.save(model, f'{save_path}/weights/last.pt')\n",
    "    \n",
    "        if(test_acc > best_test_acc):\n",
    "            torch.save(model, f'{save_path}/weights/best.pt')\n",
    "            best_test_acc = test_acc\n",
    "    \n",
    "            print(f'Saved checkpoint related to better accuracy score: {best_test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf631c-7d7c-4bb4-8084-c3fc0519827f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
