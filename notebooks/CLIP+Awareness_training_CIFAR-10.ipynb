{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efb9327",
   "metadata": {},
   "source": [
    "## Train Awareness with CLIP encoder on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "480bbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import os\n",
    "import clip\n",
    "from awareness import awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5b8301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d721bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 1\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 100\n",
    "DYNAMIC_RAY = True\n",
    "SAVE_PATH = 'checkpoints/awareness-clip_cifar10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284570ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip model parameters: 151,277,313\n",
      "Awareness model parameters: 0\n"
     ]
    }
   ],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\")\n",
    "clip_model.eval().to(device)\n",
    "\n",
    "awareness_model = awareness.Awareness(learnable=True, dynamic_ray=True)\n",
    "awareness_model.to(device)\n",
    "\n",
    "print(\"Clip model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\n",
    "print(\"Awareness model parameters:\", f\"{int(np.sum([int(np.prod(p.shape)) for p in awareness_model.parameters()])):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf6d7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_train = CIFAR10(os.path.expanduser(\"~/.cache\"), train=True, transform=preprocess, download=True)\n",
    "cifar10_test = CIFAR10(os.path.expanduser(\"~/.cache\"), train=False, transform=preprocess, download=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    cifar10_test,\n",
    "    batch_size=int(BATCH_SIZE),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79599bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9678\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "best_train_acc = 0.0\n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        cifar10_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    awareness_model.__init__(learnable=True, dynamic_ray=True)\n",
    "    \n",
    "    clip_model.eval()\n",
    "    awareness_model.eval()\n",
    "    \n",
    "    train_loaders = [train_loader]\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "    \n",
    "        for train_loader in train_loaders:\n",
    "\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "                train_correct_preds_batches = []\n",
    "                test_correct_preds_batches = []\n",
    "\n",
    "                train_count = 0\n",
    "                test_count = 0\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    images = Variable(images.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "\n",
    "                features = clip_model.encode_image(images).float()\n",
    "                preds = awareness_model(torch.unsqueeze(features,1), set_labels=labels, update_ref_insts=True)\n",
    "\n",
    "                train_correct_preds_batch = np.sum(preds.cpu().numpy() == labels.cpu().numpy())\n",
    "                train_correct_preds_batches.append(train_correct_preds_batch)\n",
    "                train_count = train_count+len(images)\n",
    "\n",
    "                references = awareness_model.awareness.ref_insts\n",
    "                references_labels = awareness_model.awareness.ref_insts_labels\n",
    "\n",
    "                n_ref_insts = len(references)\n",
    "\n",
    "                train_acc = round(np.sum(train_correct_preds_batches)/train_count, 4)\n",
    "                \n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                images = Variable(images.cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "\n",
    "            features = clip_model.encode_image(images).float()\n",
    "            preds = awareness_model(torch.unsqueeze(features,1))\n",
    "\n",
    "            test_correct_preds_batch = np.sum(preds.cpu().numpy() == labels.cpu().numpy())\n",
    "            test_correct_preds_batches.append(test_correct_preds_batch)\n",
    "            test_count = test_count+len(images)\n",
    "\n",
    "            test_batch_accuracy = round(np.sum(test_correct_preds_batch)/preds.size(0), 4)\n",
    "\n",
    "            running_test_accuracy = round(np.sum(test_correct_preds_batches)/test_count, 4)\n",
    "\n",
    "        test_acc = round(np.sum(test_correct_preds_batches)/test_count, 4)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Reference instances (N): {n_ref_insts}, Train accuracy: {train_acc}, Test accuracy: {test_acc}')\n",
    "\n",
    "        if test_accuracy > best_accuracy:\n",
    "\n",
    "            folder_path = SAVE_PATH\n",
    "\n",
    "            if(not os.path.exists(folder_path)):\n",
    "                os.mkdirs(folder_path)\n",
    "\n",
    "            torch.save(clip_model, f'./{folder_path}/clip.pt') \n",
    "            torch.save(awareness_model, f'{folder_path}/awareness.pt')\n",
    "\n",
    "            best_accuracy = test_accuracy\n",
    "\n",
    "            print(f'Saved checkpoint: epoch {epoch+1}, Reference instances (N): {n_ref_insts}, Train accuracy: {train_acc}, Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628e618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
